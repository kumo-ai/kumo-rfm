{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY4aQmWknkgx"
   },
   "source": [
    "# The Ultimate `KumoRFM` Handbook\n",
    "\n",
    "> *All you need to know or may not know yet!*\n",
    "\n",
    "The [**`kumoai.experimental.rfm`**](https://kumo-ai.github.io/kumo-sdk/docs/modules/experimental_rfm.html) package provides an implementation of [**`KumoRFM`**](https://kumo.ai/research/kumo_relational_foundation_model.pdf) that aims for\n",
    "\n",
    "* **low friction** in setting up tables, graphs and predictive queries\n",
    "* **parity** with the enterprise product\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"https://kumo-sdk-public.s3.us-west-2.amazonaws.com/rfm-colabs/infrastructure.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "Most of the actual computation actually happens on the client side, which means that you can quickly test out new ideas *without* going through enterprise and excessive AutoML, *e.g.*:\n",
    "* *What modifications does my raw data need to fit into the Kumo setting?*\n",
    "* *Are larger hops beneficial?*\n",
    "* *Can I improve my graph schema to model the task more precisely?*\n",
    "* *Does my predictive query compute the labels I expect?*\n",
    "* *How does my model behave across different anchor times?*\n",
    "\n",
    "Given reasonable graph size (*i.e.* `<500M` rows), we can expect to answer these questions **within a minute** with `KumoRFM`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1A2JZ0eRsNGp"
   },
   "source": [
    "Let's get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S67n8qHqnhnv",
    "outputId": "81e0e5da-b906-4863-93f8-b5322489983a"
   },
   "outputs": [],
   "source": [
    "!pip install kumoai --pre\n",
    "!pip install relbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsmUW2D3spgE"
   },
   "outputs": [],
   "source": [
    "import kumoai.experimental.rfm as rfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "wte__kmJsWPY",
    "outputId": "2acfbeae-27db-4e0d-e51e-96387d9cf276"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.environ.get(\"KUMO_API_KEY\"):\n",
    "    rfm.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pKDekbVtsiWY",
    "outputId": "1cc8dd60-f79a-4c74-8ab6-411575a99ee7"
   },
   "outputs": [],
   "source": [
    "rfm.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HV5WOpv3s6_T"
   },
   "source": [
    "## Graph Schema Definition\n",
    "\n",
    "`KumoRFM` comes with a type and link inference logic that infers **semantic types**, **primary keys**, **time columns**, and **graph links**, *e.g.*:\n",
    "\n",
    "* **Primary keys** are inferred based on name matching and fraction of unique values:\n",
    "\n",
    "  > *Table `USERS` ➡️ `USER_ID` is likely a primary key*\n",
    "\n",
    "  > **However**: *Table `LOCATION` ➡️ `REGION_ID` is likely **not** a primary key*\n",
    "\n",
    "* **Time columns** are inferred based on their capability to be parsed to `datetime` format:\n",
    "\n",
    "  > A column with values such as `\"2025-01-01\"` is likely a time column\n",
    "\n",
    "  In cases of multiple time columns, `KumoRFM` will select the column with the latest date, *e.g.*, we choose `CREATE_DATE` over `UPDATE_DATE`\n",
    "\n",
    "* **Semantic types** are inferred by gathering statistics from a sample:\n",
    "\n",
    "  > A column likely holds `multicategorical` data if it contains a valid separator (*e.g.*, `;`, `:`, `|` or `\\t`) and the number of unique categories when splitting is smaller than the number of unique categories without splitting.\n",
    "\n",
    "  > An integer column likely holds `categorical` rather than `numerical` data if its unique cardinality is small.\n",
    "\n",
    "  We support the following semantic types:\n",
    "\n",
    "| Type | Explanation | Example |\n",
    "|------|-------------|---------|\n",
    "| `\"numerical\"` | Numerical values (*e.g.*, `price`, `age`) | `25`, `3.14`, `-10` |\n",
    "| `\"categorical\"` | Discrete categories with limited cardinality | Color: `\"red\"`, `\"blue\"`, `\"green\"` (one cell may only have one category) |\n",
    "| `\"multicategorical\"` | Multiple categories in a single cell | `\"Action\\|Drama\\|Comedy\"`, `\"Action\\|Thriller\"` |\n",
    "| `\"ID\"` | An identifier, *e.g.*, primary keys or foreign keys | `user_id: 123`, `product_id: PRD-8729453` |\n",
    "| `\"text\"` | Natural language text | Descriptions |\n",
    "| `\"timestamp\"` | Specific point in time | `\"2025-07-11\"`,  `\"2023-02-12 09:47:58`\" |\n",
    "| `\"sequence\"` | Custom embeddings or sequential data  | `[0.25, -0.75, 0.50, ...]` |\n",
    "\n",
    "* **Graph links** are inferred based on name matching and value overlap:\n",
    "\n",
    "  > *`ORDERS.USER_ID` ↔️ `USERS.ID` is likely a valid link*\n",
    "\n",
    "**Note:** It is hard to guaranteee that type and graph inference work in 100% of all cases, and is expected to be improved constantly. If you see something unexpected, create as many issues as you can!\n",
    "\n",
    "**Note:** Schema inference is conservative, *e.g.*, we prefer to not infer a primary key rather than to infer a wrong one!\n",
    "\n",
    "**Note:** Not all semantic types are valid for all underlying data types, *e.g.*, we cannot assign the semantic type `\"text\"` to a column holding integer values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JD_qi68xvRR"
   },
   "source": [
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "id": "9IopquYAxt7P",
    "outputId": "45748d77-1de6-4e42-839c-35d95d53b37b"
   },
   "outputs": [],
   "source": [
    "from relbench.datasets import get_dataset\n",
    "\n",
    "db = get_dataset('rel-f1', download=True).get_db(upto_test_timestamp=False)\n",
    "df_dict = {name: table.df for name, table in db.table_dict.items()}\n",
    "# Call `from_data(infer_metadata=False)` if you do not want to infer anything!\n",
    "graph = rfm.LocalGraph.from_data(df_dict, edges=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgeFt7Hs06dD"
   },
   "source": [
    "We can see that `KumoRFM` correctly identified all primary keys and time columns in the `rel-f1` dataset. Correctly specifying primary keys and time columns is the most important part while setting up your data since it defines how tables can be linked and which temporal information is available for a given anchor time.\n",
    "\n",
    "If you ever need to change/add primary keys or time columns, you can simply do this via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4855sN8D1tuH"
   },
   "outputs": [],
   "source": [
    "graph['qualifying'].primary_key = 'qualifyId'\n",
    "graph['races'].time_column = 'date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPrx2gX_2AuR"
   },
   "source": [
    "One primary keys are set up, we can start linking tables together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "7WLOiW2D2AKV",
    "outputId": "4bf7f0c2-c72d-44a2-cfaa-5e95a6602480"
   },
   "outputs": [],
   "source": [
    "graph.infer_links();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "Ikb0seozzte_",
    "outputId": "bdb8faca-707d-40c1-b229-c8e0e7bd9dd2"
   },
   "outputs": [],
   "source": [
    "graph.visualize(show_columns=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkQdTGIW24Pv"
   },
   "source": [
    "Let's also double check the columns and semantic types in our tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "2PL2cTQQ27tO",
    "outputId": "1fa4d86b-d059-4d43-868c-53d0a28b6336"
   },
   "outputs": [],
   "source": [
    "display(df_dict['drivers'].head())\n",
    "graph['drivers'].print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jPrR6KR5Z8o"
   },
   "source": [
    "This looks mostly good, but we actually see a lot of duplicated information in what is stored in these columns. For the underlying prediction task, we likely don't need `driverRef`, `code`, `forename` and `surname` all simultaneously. Let's clean this up a bit and only maintain `driverRef`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "1Tv8PjHW3Lzc",
    "outputId": "412b5cea-6f21-410f-f916-deb01953d630"
   },
   "outputs": [],
   "source": [
    "del graph['drivers']['code']\n",
    "del graph['drivers']['forename']\n",
    "del graph['drivers']['surname']\n",
    "\n",
    "graph['drivers'].print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2-XTNW43LNk"
   },
   "source": [
    "This looks better. Let's look at another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "pOSDhd546H5s",
    "outputId": "308d7479-fbd5-4826-e4fb-d8ac29e616d5"
   },
   "outputs": [],
   "source": [
    "display(df_dict['standings'].head())\n",
    "graph['standings'].print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mtk7eqfz6X68"
   },
   "source": [
    "This looks alright as well. One could argue that `wins` and `points` should actually be of type `numerical` but are inferred as `categorical` due to their low cardinality. Let's change this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "1zUtpenK6XbJ",
    "outputId": "33c04b54-1577-411c-a893-a4bd346bc1e0"
   },
   "outputs": [],
   "source": [
    "graph['standings']['wins'].stype = 'numerical'\n",
    "graph['standings']['points'].stype = 'numerical'\n",
    "graph['standings'].print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlyk_6NesxAH"
   },
   "source": [
    "You may note that it is always good to double-check the underlying data and inference logic. **Ensuring to operate on correctly defined data is the most important step in the whole Kumo pipeline!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emNswoUGsaV5"
   },
   "source": [
    "The real beauty of the in-memory mode of `KumoRFM` is that we are not bounded by a pre-determined data layout. We can simply change our graph as we like, and see its impact a few seconds later. `KumoRFM` lets you iterate fast, and thus can be both seen as **(1)** an exceptional playground to test out ideas and **(2)** having a training-free prediction workflow.\n",
    "\n",
    "For example, let's just remove `constructor` and `circuit` tables and see how the model behaves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iyJzEs58sE9"
   },
   "outputs": [],
   "source": [
    "del graph['constructors']\n",
    "del graph['constructor_standings']\n",
    "del graph['constructor_results']\n",
    "del graph['circuits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "BpkUC5Sg81lo",
    "outputId": "cd127129-e486-495e-fe93-d377659567a5"
   },
   "outputs": [],
   "source": [
    "graph.visualize(show_columns=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzFhprlh_PiC"
   },
   "source": [
    "Furthermore, we could simply merge `results` and `standings` tables into a single table since they have a 1-to-1 mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tY4VafVj9oEY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.merge(\n",
    "    df_dict['results'],\n",
    "    df_dict['standings'],\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='left',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyhkmXUQAjxR"
   },
   "source": [
    "## Predictive Queries\n",
    "\n",
    "Once we are satisfied with our graph, we can initialize our model operating on this graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "7962ed8f081f4f98b8e5810d41423cf3",
      "d695db24b17b445e8afcfe95d6f5f636"
     ]
    },
    "id": "5_6cQFynAsOG",
    "outputId": "666afb03-7d36-4c4c-f122-62e4482dfdec"
   },
   "outputs": [],
   "source": [
    "model = rfm.KumoRFM(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uWsX5JrAyg8"
   },
   "source": [
    "**Note:** It is always good to double-check the logs. Look out for expected timestamp information and graph sizes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIvuHCEtBBx-"
   },
   "source": [
    "We are now ready to write **predictive queries**:\n",
    "\n",
    "`KumoRFM` aims for parity with PQL in Kumo enterprise, but ...\n",
    "\n",
    "* `PREDICT <TARGET> FOR EACH <ENTITY>` needs to be replaced by `PREDICT <TARGET> FOR <ENTITY> IN (<ID>)`: `KumoRFM` is not a batch processor, we make predictions for individual entities!\n",
    "* The following PQ features are not supported *(yet)*:\n",
    "  * 2-hop target filters, *e.g.*, `LIST_DISTINCT(orders.item_id WHERE items.category = \"...\", 0, 7)`\n",
    "  * 1-hop static targets, *e.g.*, `PREDICT customer_info.AGE FOR customer.ID`\n",
    "  * Static llink prediction\n",
    "  * Forecasting\n",
    "  * Multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoilJH8-Fksu"
   },
   "source": [
    "Let's predict whether a driver will qualify in the top-3 for a race within the next 30 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eeQbtw9BBWL"
   },
   "outputs": [],
   "source": [
    "query = (\"PREDICT COUNT(qualifying.* WHERE qualifying.position <= 3, 0, 30, days)>0 \"\n",
    "         \"FOR drivers.driverId IN ({indices})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en4JiW4sF_aZ"
   },
   "source": [
    "`KumoRFM` allows us to inspect the resulting training table for a given `anchor_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "8tGGOPbsAx7C",
    "outputId": "1b871cb0-be09-4e2c-d046-e6fa6a16621e"
   },
   "outputs": [],
   "source": [
    "train_table = model.get_train_table(\n",
    "    query.format(indices='0, 1'),  # Dummy values.\n",
    "    size=1000,\n",
    "    anchor_time=pd.Timestamp('2013-03-16'),\n",
    ")\n",
    "display(train_table)\n",
    "display(train_table['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTfnPO90GiOt"
   },
   "source": [
    "Oh, it looks like something went wrong. We unexpectedly see very skewed label distribution, and the training table actually contains data for a total of 857 drivers.\n",
    "\n",
    "It looks like our predictive query is a little ill-defined. We need to filter by **active drivers**, *i.e.* we want to only consider drivers that *can* actually qualify. As such, a better way to write this query is via `MIN` aggregation, which will automatically drop any entity without any event within its time window (alternatively, we can achieve the same via the `ASSUMING` clause):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P47P-DtdAjGB"
   },
   "outputs": [],
   "source": [
    "query = (\"PREDICT MIN(qualifying.position, 0, 30, days)<=3 \"\n",
    "         \"FOR drivers.driverId IN ({indices})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "id": "uZN4jFfJPmhi",
    "outputId": "4135e23c-5cf3-44b5-f285-0bf902b8ac81"
   },
   "outputs": [],
   "source": [
    "train_table = model.get_train_table(\n",
    "    query.format(indices='0, 1'),  # Dummy values.\n",
    "    size=1000,\n",
    "    anchor_time=pd.Timestamp('2013-03-16'),\n",
    ")\n",
    "display(train_table)\n",
    "display(train_table['TARGET'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2QMbH1BAimX"
   },
   "source": [
    "This looks better! Now let's make an actual prediction. Let's look up valid drivers from the `qualifying` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811,
     "referenced_widgets": [
      "3e030e671dce4b0b8e9703eeb75f3c8c",
      "f24e4cdacf5246908e6731c866d4238d"
     ]
    },
    "id": "nptUjRm_SU7P",
    "outputId": "662f731f-3b08-481d-ca53-7f345169aa5c"
   },
   "outputs": [],
   "source": [
    "indices = df_dict['qualifying'].sort_values('date', ascending=False)['driverId'].iloc[:20]\n",
    "\n",
    "result = model.predict(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efTR6uNhUGeD"
   },
   "source": [
    "Yeah, we got a prediction, but there is also a warning. The warning tells us that `KumoRFM` by default could only found 262 in-context examples, which is far less than it expected (1000). Such situation can naturally arise in `KumoRFM` due to the way online Predictive Query works:\n",
    "\n",
    "1. We sample a set of entities for a given timestamp\n",
    "1. We compute their respective labels\n",
    "1. We drop any entities with invalid labels (*e.g.*, inactive drivers)\n",
    "1. We repeat this process until we have found 1000 labels or we have reached 20 iterations\n",
    "\n",
    "Since we have a strict entity filter here (due to the `MIN` aggregation type), 20 iterations of PQ only yield us 262 labels. In these cases, we can tell `KumoRFM` to increase its search window via the **`max_pq_iterations`** option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745,
     "referenced_widgets": [
      "a610060d2ef145ae8dd4d1a3c5c6e251",
      "2fdca2d90ec34b9c8ead913c18f42d45"
     ]
    },
    "id": "FnUXeEtxUajD",
    "outputId": "3e3cc108-2af9-4f9c-e20f-51f6e0ff5cf8"
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    "    max_pq_iterations=200,\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKt9MHKGVLwj"
   },
   "source": [
    "This looks better. We have now successfully found 1000 in-context examples.\n",
    "\n",
    "There exists a few **other options** we can tune as part of `KumoRFM`:\n",
    "\n",
    "* **`run_mode`**: Defines the trade-off between runtime and model performance (`\"fast\"`, `\"normal\"`, `\"best\"`). By default, this is set to `run_mode=\"fast\"`. Increasing the `run_mode` will gather **(1)** more in-context examples and **(2)** use a \"more expensive\" model variant.\n",
    "\n",
    "| `run_mode` | #in-context examples |\n",
    "| - | - |\n",
    "| `\"fast\"` | 1,000 |\n",
    "| `\"normal\"` | 5,000 |\n",
    "| `\"best\"` | 10,000 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745,
     "referenced_widgets": [
      "90c8f516548e4b5aa7d09f07ccaecb27",
      "6c92271f94254806abdea285707e1c9d"
     ]
    },
    "id": "I5_J3DVAVWMp",
    "outputId": "0c747e5b-e05e-48fb-e024-101e65632fcc"
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    "    max_pq_iterations=200,\n",
    "    run_mode=\"best\",\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2PZ8WcLWVv2"
   },
   "source": [
    "* **`anchor_time`**: By default, `KumoRFM` will make a prediction for **the latest timestamp** in your data. You can customize this by explicitely specifying at which time you want to make a prediction. If this anchor timestamp is in the past, no future data will be used to derive a prediction:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"https://kumo-sdk-public.s3.us-west-2.amazonaws.com/rfm-colabs/predict_time.png\" width=\"600\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811,
     "referenced_widgets": [
      "96715133463548bf8352e2b6bedbcf3d",
      "3efdb5f36b114c468f79e4730e33f7cb"
     ]
    },
    "id": "-7-DIXHHWhwa",
    "outputId": "bba7b21d-8513-4043-d08e-d23af5284476"
   },
   "outputs": [],
   "source": [
    "model.predict(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    "    max_pq_iterations=200,\n",
    "    run_mode=\"best\",\n",
    "    anchor_time=pd.Timestamp('2025-01-01'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nETNKuj3W9bm"
   },
   "source": [
    "The warning tells us that the `anchor_time` is too far in the future, which will decrease model performance dramatically (since we are basically lacking 2 years of data here). Let's use a better `anchor_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745,
     "referenced_widgets": [
      "aac99fce00014ed4944217f3f84084c7",
      "bc9d1eec64c440b29a2af1fb19b1628a"
     ]
    },
    "id": "oZaFE3fiXJGh",
    "outputId": "b13424c1-5a93-47d4-d4fa-7f87f52e562c"
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    "    max_pq_iterations=200,\n",
    "    run_mode=\"best\",\n",
    "    anchor_time=pd.Timestamp('2023-03-16'),\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kj6zz7t0Xkst"
   },
   "source": [
    "* **`num_neighbors`**: Often times, it is desired to have more fine-grained control over how subgraphs are formed. For example, we can decrease the number of neighbors to be sampled in each hop to combat oversmoothing, or increase the number of neighbors to be sampled in each hop to be able to look at a larger time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745,
     "referenced_widgets": [
      "13d753c462c24c4c85649ff0cc54db8d",
      "a418e5a249a64891a28075f136a15697"
     ]
    },
    "id": "kqrpyFr6XVaZ",
    "outputId": "0fa6a33f-c82a-4c07-dc28-693d0d100d8e"
   },
   "outputs": [],
   "source": [
    "result = model.predict(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    "    max_pq_iterations=200,\n",
    "    run_mode=\"best\",\n",
    "    anchor_time=pd.Timestamp('2023-03-16'),\n",
    "    num_neighbors=[8, 8],\n",
    ")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q_tkafxXx9O"
   },
   "source": [
    "Finally, we can evaluate how our model behaves *w.r.t.* metrics. For this, `KumoRFM` provides the `evaluate` function, which evaluates on a historical snapshot for which ground-truth labels are available:\n",
    "\n",
    "<div align=\"left\">\n",
    "  <img src=\"https://kumo-sdk-public.s3.us-west-2.amazonaws.com/rfm-colabs/evaluate_time.png\" width=\"700\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "referenced_widgets": [
      "cd7e7b2406e24a51954ef007aa003a78",
      "fdf2fdbd6d2740f292733f4adb706919"
     ]
    },
    "id": "dX2JnKu4Xxr1",
    "outputId": "0299d9bf-a699-4e68-ac56-bfa1846c6259"
   },
   "outputs": [],
   "source": [
    "metrics = model.evaluate(\n",
    "    query.format(indices=', '.join(str(i) for i in indices)),\n",
    "    max_pq_iterations=200,\n",
    "    run_mode=\"best\",\n",
    "    anchor_time=pd.Timestamp('2023-03-16'),\n",
    "    num_neighbors=[8, 8],\n",
    ")\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J6YReOwZK1J"
   },
   "source": [
    "**A final note on context size:**\n",
    "\n",
    "Every predictive query returns the size of the context that is send to the server. The context size has a current limit of **30MB**. In most cases, this should be absolutely sufficient (30MB is tons of data for a single prediction!!!)\n",
    "\n",
    "If a query exceeds this limit, then this happens either because of:\n",
    "1. Too many context examples ➡️ reduce the `run_mode`\n",
    "1. Subgraphs are too big ➡️ reduce neighbor sampling parameters\n",
    "1. Too many tables/too many columns ➡️ reduce the number of tables or the width of tables, *e.g.*, it may not be necessary to send entire posts or comments for ~100k threads over the wire in order to make a single prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mM3lgmZxZON8"
   },
   "source": [
    "## In-depth Evaluation\n",
    "\n",
    "Often times, it may be required to evaluate `KumoRFM` against a held out training and test set. `KumoRFM` has full support for **custom training tables** via static PQs.\n",
    "\n",
    "The idea is simple: We attach a context table to the graph which contains the training and test entities, their timestamps, and training labels. For the test entities, we **keep their labels blank** and ask `KumoRFM` to infer them.\n",
    "With this scheme, we can support any classification or regressions task by interpreting them as **missing value imputation**.\n",
    "\n",
    "For `rel-bench`, we can do this by concatenating the task-specific training tables across the different splits, but ensure that we mask out all final test labels to prevent leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZZYwn6savCr",
    "outputId": "171c9687-9bec-4239-c568-c28ba5829597"
   },
   "outputs": [],
   "source": [
    "from relbench.tasks import get_task\n",
    "\n",
    "task = get_task('rel-f1', 'driver-top3', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "id": "22qT-6VmayLA",
    "outputId": "345c3248-3370-43e2-fc09-9f8bd0f15f80"
   },
   "outputs": [],
   "source": [
    "# Concatenate training labels into a single `pd.DataFrame`:\n",
    "context_dfs = []\n",
    "for split in ['test', 'val', 'train']:\n",
    "    df = task.get_table(split, mask_input_cols=False).df\n",
    "    df = df.drop(columns='index', errors='ignore')\n",
    "    df[task.target_col] = df[task.target_col].astype('Int64')\n",
    "    if split == 'test':\n",
    "        df[task.target_col] = None  # Do not leak test labels.\n",
    "    context_dfs.append(df)\n",
    "context_df = pd.concat(context_dfs, axis=0, ignore_index=True).reset_index()\n",
    "\n",
    "# Assign this `pd.DataFrame` to a table in the graph:\n",
    "context_table = rfm.LocalTable(\n",
    "    context_df,\n",
    "    name='context',\n",
    "    primary_key='index',\n",
    "    time_column=task.time_col,\n",
    ")\n",
    "graph.add_table(context_table)\n",
    "graph.link(context_table.name, task.entity_col, task.entity_table)\n",
    "\n",
    "context_table.print_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYVZGEZdbTIz"
   },
   "source": [
    "Note that we have added a **primary key** `index` to the context table - a unique identifier to be able to reference any row within that table.\n",
    "This is needed to be able to later predict for individual rows within that table.\n",
    "\n",
    "We are now ready to evaluate `KumoRFM` on the `driver-top3` task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "262662d44000463da056e9a119a543c6",
      "e3ca584493f14165a73293c608d4b3b1"
     ]
    },
    "id": "Y_S2OPOPbSkb",
    "outputId": "7927477c-426f-4d6b-c296-32a60fb25ac1"
   },
   "outputs": [],
   "source": [
    "model = rfm.KumoRFM(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayvYAQUkbbfM"
   },
   "source": [
    "Our predictive query now simply performs missing value imputation for the target column in the context table. In order to cast this into a binary classification problem, we explicitely add a condition to the target clause (*i.e.* `target_col = 1`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vqu-tvsSbd5M"
   },
   "outputs": [],
   "source": [
    "query = (f\"PREDICT context.{task.target_col} = 1 \"\n",
    "         f\"FOR EACH context.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make use of `KumoRFM.batch_mode` context manager to predict in batches! It allows you to provide a PQ along with a list of indices to predict for, which in our case is all the rows of `test_df` - the batch mode context manager then takes care of batching, and retry logic ensuring that we get all of our predictions smoothly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test dataframe\n",
    "test_df = task.get_table('test', mask_input_cols=False).df\n",
    "\n",
    "# Predict in batches for the test set\n",
    "with model.batch_mode(batch_size='max', num_retries=1):\n",
    "  df = model.predict(\n",
    "      query=query,\n",
    "      indices=np.arange(len(test_df)),\n",
    "      run_mode='best',  # Trades runtime in favor of better model performance.\n",
    "      anchor_time='entity',  # Use context table time as anchor time.\n",
    "      num_neighbors=[1, 8, 8],\n",
    "      verbose=True,\n",
    "  )\n",
    "\n",
    "# Get the predicted probabilities for the test set\n",
    "y_pred = df['True_PROB'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJpq-pWXbotO"
   },
   "source": [
    "There exists a few things to look out for in the code snippet above:\n",
    "\n",
    "* The **`anchor_time`** supports two options: **(1)** a `pd.Timestamp(...)` that denotes the absolute time for when we are making a prediction; **(2)** the string `\"entity\"`, which tells `KumoRFM` to use the timestamp of the entity as its individual anchor timestamp. In this case, we use `anchor_time=\"entity\"` since we want to make sure that we respect the anchor times given by us as part of the context table.\n",
    "* Since we added an additional context table to the graph, it is good practice to increase the number of hops in our subgraphs from 2 (default) to 3 (since we first need to go from `context` to `drivers`). We can specify this via the **`num_neighbors`** option (`len(num_neighbors) == 3`).\n",
    "* The **`verbose`** option ensures that we do not log within every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrrQl-YHbxiT"
   },
   "source": [
    "Finally, we can compute metrics based on the given ground-truth information from the `rel-bench` test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhJnf-3sbziy",
    "outputId": "398b2fe9-ea1a-4003-e2be-6f8212eb1728"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_test = test_df[task.target_col].to_numpy().astype(int)[:len(y_pred)]\n",
    "print(f'AUROC: {roc_auc_score(y_test, y_pred):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxOtLC_Rb3S0"
   },
   "source": [
    "This setup lets us benchmark any task with an external training and test set without further modifications required."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13d753c462c24c4c85649ff0cc54db8d": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_a418e5a249a64891a28075f136a15697",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (3.23s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 3,111 in-context examples with 19.29% positive cases                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 0.56MB                                                                             </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (3.23s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                               \u001b[0m\n   \u001b[2m↳ Collected 3,111 in-context examples with 19.29% positive cases                                               \u001b[0m\n   \u001b[2m↳ Generated context of size 0.56MB                                                                             \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "262662d44000463da056e9a119a543c6": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_e3ca584493f14165a73293c608d4b3b1",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000\">Materializing graph (0.04s)                              </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Sanitized input data                                   </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected primary keys from 6 tables                   </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal graph from 1896-12-28 to 2023-11-26</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Created graph with 74,644 nodes and 285,410 edges      </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[32mMaterializing graph (0.04s)                              \u001b[0m\n   \u001b[2m↳ Sanitized input data                                   \u001b[0m\n   \u001b[2m↳ Collected primary keys from 6 tables                   \u001b[0m\n   \u001b[2m↳ Identified temporal graph from 1896-12-28 to 2023-11-26\u001b[0m\n   \u001b[2m↳ Created graph with 74,644 nodes and 285,410 edges      \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "2fdca2d90ec34b9c8ead913c18f42d45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34677e2f30f04db5927dce1a8027c0f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e030e671dce4b0b8e9703eeb75f3c8c": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_f24e4cdacf5246908e6731c866d4238d",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (1.13s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 262 in-context examples with 23.66% positive cases                                                 </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 0.15MB                                                                             </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (1.13s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                               \u001b[0m\n   \u001b[2m↳ Collected 262 in-context examples with 23.66% positive cases                                                 \u001b[0m\n   \u001b[2m↳ Generated context of size 0.15MB                                                                             \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "3efdb5f36b114c468f79e4730e33f7cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c92271f94254806abdea285707e1c9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7962ed8f081f4f98b8e5810d41423cf3": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_d695db24b17b445e8afcfe95d6f5f636",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000\">Materializing graph (0.20s)                              </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Sanitized input data                                   </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected primary keys from 5 tables                   </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal graph from 1896-12-28 to 2023-11-26</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Created graph with 71,977 nodes and 280,076 edges      </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[32mMaterializing graph (0.20s)                              \u001b[0m\n   \u001b[2m↳ Sanitized input data                                   \u001b[0m\n   \u001b[2m↳ Collected primary keys from 5 tables                   \u001b[0m\n   \u001b[2m↳ Identified temporal graph from 1896-12-28 to 2023-11-26\u001b[0m\n   \u001b[2m↳ Created graph with 71,977 nodes and 280,076 edges      \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "90c8f516548e4b5aa7d09f07ccaecb27": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_6c92271f94254806abdea285707e1c9d",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (4.62s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 3,143 in-context examples with 19.47% positive cases                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 1.88MB                                                                             </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (4.62s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                               \u001b[0m\n   \u001b[2m↳ Collected 3,143 in-context examples with 19.47% positive cases                                               \u001b[0m\n   \u001b[2m↳ Generated context of size 1.88MB                                                                             \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "96715133463548bf8352e2b6bedbcf3d": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_3efdb5f36b114c468f79e4730e33f7cb",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (4.55s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 2,864 in-context examples with 19.83% positive cases                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 1.80MB                                                                             </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (4.55s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                               \u001b[0m\n   \u001b[2m↳ Collected 2,864 in-context examples with 19.83% positive cases                                               \u001b[0m\n   \u001b[2m↳ Generated context of size 1.80MB                                                                             \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "a418e5a249a64891a28075f136a15697": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a610060d2ef145ae8dd4d1a3c5c6e251": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_2fdca2d90ec34b9c8ead913c18f42d45",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (1.68s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 1,000 in-context examples with 20.50% positive cases                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 0.39MB                                                                             </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (1.68s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                               \u001b[0m\n   \u001b[2m↳ Collected 1,000 in-context examples with 20.50% positive cases                                               \u001b[0m\n   \u001b[2m↳ Generated context of size 0.39MB                                                                             \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "aac99fce00014ed4944217f3f84084c7": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_bc9d1eec64c440b29a2af1fb19b1628a",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (5.05s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 3,111 in-context examples with 19.29% positive cases                                               </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 1.86MB                                                                             </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (5.05s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                               \u001b[0m\n   \u001b[2m↳ Collected 3,111 in-context examples with 19.29% positive cases                                               \u001b[0m\n   \u001b[2m↳ Generated context of size 1.86MB                                                                             \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "bc9d1eec64c440b29a2af1fb19b1628a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd7e7b2406e24a51954ef007aa003a78": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_fdf2fdbd6d2740f292733f4adb706919",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">EVALUATE</span><span style=\"color: #008000; text-decoration-color: #008000\"> </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">MIN</span><span style=\"color: #008000; text-decoration-color: #008000\">(qualifying.position, 0, 30, days)&lt;=3 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> drivers.driverId </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (807, 855, 452, 824, 821, ...) (3.46s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified temporal binary classification task                                                                </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 20 test examples                                                                                    </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 3,111 in-context examples with 19.29% positive cases                                                </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 0.58MB                                                                              </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mEVALUATE\u001b[0m\u001b[32m \u001b[0m\u001b[1;32mMIN\u001b[0m\u001b[32m(qualifying.position, 0, 30, days)<=3 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m drivers.driverId \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (807, 855, 452, 824, 821, ...) (3.46s)\u001b[0m\n   \u001b[2m↳ Identified temporal binary classification task                                                                \u001b[0m\n   \u001b[2m↳ Collected 20 test examples                                                                                    \u001b[0m\n   \u001b[2m↳ Collected 3,111 in-context examples with 19.29% positive cases                                                \u001b[0m\n   \u001b[2m↳ Generated context of size 0.58MB                                                                              \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "d695db24b17b445e8afcfe95d6f5f636": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d713397d590d4e418bdd3baaf8051678": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_34677e2f30f04db5927dce1a8027c0f9",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">✅</span> <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">PREDICT</span><span style=\"color: #008000; text-decoration-color: #008000\"> context.qualifying=1 </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">FOR</span><span style=\"color: #008000; text-decoration-color: #008000\"> context.index </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">IN</span><span style=\"color: #008000; text-decoration-color: #008000\"> (0, 1, 2, 3, 4, ...) (1.86s)</span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Identified static binary classification task                                </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Collected 1,941 in-context examples with 18.03% positive cases              </span>\n   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">↳ Generated context of size 0.62MB                                            </span>\n</pre>\n",
         "text/plain": "\u001b[32m✅\u001b[0m \u001b[1;32mPREDICT\u001b[0m\u001b[32m context.qualifying=1 \u001b[0m\u001b[1;32mFOR\u001b[0m\u001b[32m context.index \u001b[0m\u001b[1;32mIN\u001b[0m\u001b[32m (0, 1, 2, 3, 4, ...) (1.86s)\u001b[0m\n   \u001b[2m↳ Identified static binary classification task                                \u001b[0m\n   \u001b[2m↳ Collected 1,941 in-context examples with 18.03% positive cases              \u001b[0m\n   \u001b[2m↳ Generated context of size 0.62MB                                            \u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "e3ca584493f14165a73293c608d4b3b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f24e4cdacf5246908e6731c866d4238d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdf2fdbd6d2740f292733f4adb706919": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
